{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络（RNN, Recurrent Neural Network）\n",
    "循环神经网络是一种高效的自然语言处理及其它的序列模型的网络结构，因为它有一种特殊的“记忆”结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-序列模型的应用：\n",
    "1. 语音识别\n",
    "2. 音乐发生器\n",
    "3. 情感分析\n",
    "4. DNA序列分析\n",
    "5. 机器翻译\n",
    "6. 视频动作识别\n",
    "7. 命名实体识别\n",
    "<img src=\"images/example_of_sequence_data.png\" style=\"width:500px;height:300px\">\n",
    "*注:* 循环神经网络多用于语言序列，也有人使用其进行股票分析，故障诊断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- RNN网络模型\n",
    "循环神经网络模型\n",
    "<img src=\"images/Recurrent_Neural_Networks_model.png\">\n",
    "### 2.1-RNN网络单元\n",
    "\n",
    "循环神经网络可以看成是单个单元的重复，首先看一下每个单元的操作\n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "**计算过程**\n",
    "1. 计算隐藏层的激活值: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. 使用新的隐藏状态 $a^{\\langle t \\rangle}$, 计算预测输出 $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. 使用自定义函数: `softmax`.\n",
    "3. 存储 cache包括$(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$  \n",
    "4. 返回 $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ 和 cache\n",
    "\n",
    " $m$：实例的个数. 因此, $x^{\\langle t \\rangle}$ 的维度为 $(n_x,m)$,  $a^{\\langle t \\rangle}$ 的维度 $(n_a,m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#定义softmax函数\n",
    "def softmax(x):\n",
    "    '''\n",
    "    softmax激活函数，加入非线性因素\n",
    "    每一列为一个样本\n",
    "    参数：\n",
    "        x:输入的类别\n",
    "    输出：\n",
    "    '''\n",
    "    exp_x = np.exp(x-np.max(x))\n",
    "    return exp_x/exp_x.sum(axis=0)\n",
    "\n",
    "def rnn_cell_forward(xt,a_prev,parameters):\n",
    "    '''\n",
    "    实现一个RNN单元的前向传递\n",
    "    输入：\n",
    "        xt：时刻t的输入数据，数据类型：numpy.array,大小:(n_x,m)\n",
    "        a_prev: t-1时刻的隐藏层，数据类型：numpy.array，大小:(n_a,m)\n",
    "        paremeters:Python的一个字典类型数据包括：\n",
    "            Wax:乘以输入的权重矩阵，数据类型numpy.array,大小：(n_a,n_x)\n",
    "            Waa:乘以隐藏层的权重矩阵，数据类型numpy.array,大小（n_a,n_a）\n",
    "            Wya:隐藏层到输出层的相关矩阵，数据类型numpy.array,大小：(n_y,n_a)\n",
    "            ba:偏置，数据类型numpy.array，大小：(n_a,1)\n",
    "            by:隐藏层到输出层偏置，数据类型numpy.array，大小：(n_y,1)\n",
    "    输出：\n",
    "        a_next:下一个隐藏层的状态:(n_a,m)\n",
    "        yt_pred:时刻t的预测（n_y,m）\n",
    "        cache:反向传递需要的元组数据，包括（a_next,a_prev,xt,parameters）    \n",
    "    '''\n",
    "    \n",
    "    #从parameters中提取参数\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    #计算下一个隐藏层的状态\n",
    "    a_next = np.tanh(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)\n",
    "    #计算输出状态\n",
    "    yt_pred = softmax(np.dot(Wya,a_next)+by)\n",
    "    #存储反向传递需要的值\n",
    "    cache = (a_next,a_prev,xt,parameters)\n",
    "    \n",
    "    return a_next,yt_pred,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "### 验证代码\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2- RNN的前向传递\n",
    "RNN由多个基本单元重复组成，每一个单元都从前一个隐藏单元中接受输入($a^{\\langle t-1 \\rangle}$)，并且输出($a^{\\langle t \\rangle}$)给下一层\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "1. 创建一个零向量 ($a$) 存储RNN计算的隐藏状态.\n",
    "2. 初始化第一个隐藏状态 $a_0$ .\n",
    "3. 随着time_step循环（$t$） :\n",
    "    - 通过调用 `rnn_step_forward`更新下一个隐藏状态\n",
    "    - 存储下一个隐藏状态 $a$ ($t^{th}$ position) \n",
    "    - 存储预测值y\n",
    "    - 把cache加入caches列表\n",
    "4. 返回 $a$, $y$ 和 caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x,a0,parameters):\n",
    "    '''\n",
    "    完成上述说明的RNN前向传递\n",
    "    输入：\n",
    "        x：每一个time_step的输入数据,(n_x,m,T_x)\n",
    "        a0:初始的隐藏状态,(n_a,m)\n",
    "        parameters:python字典数据,包含：\n",
    "            Wax:乘以输入的权重矩阵，数据类型numpy.array,大小：(n_a,n_x)\n",
    "            Waa:乘以隐藏层的权重矩阵，数据类型numpy.array,大小（n_a,n_a）\n",
    "            Wya:隐藏层到输出层的相关矩阵，数据类型numpy.array,大小：(n_y,n_a)\n",
    "            ba:偏置，数据类型numpy.array，大小：(n_a,1)\n",
    "            by:隐藏层到输出层偏置，数据类型numpy.array，大小：(n_y,1)\n",
    "    输出：\n",
    "        a:每一个time_step隐藏状态,(n_a,m,T_x)\n",
    "        y_pred:每一个time_step的预测值(n_y,m,T_x)\n",
    "        caches:反向传递所需要的元组值包括(list of caches,x)\n",
    "    '''\n",
    "    #初始化caches\n",
    "    caches = []\n",
    "    \n",
    "    #提取x和Wya的维度\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_y,n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    #初始化a和y\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    y_pred = np.zeros([n_y,m,T_x])\n",
    "    \n",
    "    #初始化a_next\n",
    "    a_next = a0\n",
    "    \n",
    "    #根据所有的time_steps循环\n",
    "    for t in range(T_x):\n",
    "        #调用rnn_cell_forward，计算a_next,yt_pred,cache\n",
    "        a_next,yt_pred,cache = rnn_cell_forward(x[:,:,t],a_next,parameters)\n",
    "        #保存新的a_next到a中\n",
    "        a[:,:,t] = a_next\n",
    "        #保存yt_pred到y中\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        #添加cache到caches中\n",
    "        caches.append(cache)\n",
    "    \n",
    "    #保存方向传递需要的所有的值到caches\n",
    "    caches = (caches,x)\n",
    "    \n",
    "    return a,y_pred,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "##调试rnn_forward代码\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-LSTM(long short-term memory)\n",
    "***解决普通的RNN梯度消失的现象***\n",
    "### 3.1- LSTM的单元结构\n",
    "<img src=\"images/LSTM.png\" style=\"width:500;height:400px;\">\n",
    "\n",
    "**遗忘门(forget gate)**\n",
    "\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "经过`sigmoid`函数$\\Gamma_f^{\\langle t \\rangle}$取值为0~1之间\n",
    "\n",
    "**更新门(update gate)**\n",
    "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "**输出门**\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "**完成以下操作**:\n",
    "1. 堆叠 $a^{\\langle t-1 \\rangle}$ 和 $x^{\\langle t \\rangle}$ 在一个矩阵中: $concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$(方便运算)\n",
    "2. 根据上述公式进行计算. `sigmoid()` 和 `np.tanh()`.\n",
    "3. 计算输出 $y^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def lstm_cell_forward(xt,a_prev,c_prev,parameters):\n",
    "    '''\n",
    "    完成lstm单元前向传递操作\n",
    "    输入：\n",
    "        xt：时刻t的输入数据，数据类型：numpy.array,大小:(n_x,m)\n",
    "        a_prev: t-1时刻的隐藏层，数据类型：numpy.array，大小:(n_a,m)\n",
    "        c_prev: t-1时刻的记忆状态，数据类型：numpy.array,大小（n_a,m）\n",
    "        paremeters:Python的一个字典类型数据包括：\n",
    "            Wf:遗忘门的权重矩阵，输类型numpy.array，大小：(n_a,n_a+n_x)\n",
    "            bf:遗忘门的偏置，输类型numpy.array，大小：(n_a,1)\n",
    "            Wi:乘以输入的权重矩阵，数据类型numpy.array,大小：(n_a,n_x+n_a)\n",
    "            Wi:更新门的偏置，数据类型numpy.array,大小：(n_a,1)\n",
    "            Wc:第一个tanh的权重，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            bc:第一个tanh的偏置，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            Wo:输出门的权重，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            bo:输出门的偏置，数据类型numpy.array,大小(n_a,1)\n",
    "            Wy:隐藏层到输出层的权重，数据类型numpy.array,大小(n_y,n_a)\n",
    "            by:隐藏层到输出层的偏置，数据类型numpy.array,大小(n_y,n_a)\n",
    "    输出：\n",
    "        a_next:下一个隐藏层的状态，(n_a,m)\n",
    "        c_next:下一个记忆状态,(n_a,m)\n",
    "        yt_pred:时刻t的输出,(n_y,m)\n",
    "        cache:方向传递的需要的元组值,包括(a_next,c_next,a_prev,c_prev,xt,parameters)\n",
    "    '''\n",
    "    \n",
    "    #从parameters中提取权重\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    #从xt和Wy中提取维度\n",
    "    n_x,m = xt.shape\n",
    "    n_y,n_a = Wy.shape\n",
    "    \n",
    "    #合并a_prev和xt\n",
    "    concat= np.zeros([n_a+n_x,m])\n",
    "    concat[:n_a,:] = a_prev\n",
    "    concat[n_a:,:] = xt\n",
    "    #concat = np.concatenate(a_prev,xt)\n",
    "    \n",
    "    #遗忘门\n",
    "    ft = sigmoid(np.dot(Wf,concat)+bf)\n",
    "    #更新门\n",
    "    it = sigmoid(np.dot(Wi,concat)+bi)\n",
    "    cct = np.tanh(np.dot(Wc,concat)+bc)\n",
    "    #输出门\n",
    "    c_next = ft*c_prev+it*cct\n",
    "    ot = sigmoid(np.dot(Wo,concat)+bo)\n",
    "    a_next = ot*np.tanh(c_next)\n",
    "    \n",
    "    #计算LSTM单元的输出值\n",
    "    yt_pred = softmax(np.dot(Wy,a_next)+by)\n",
    "    \n",
    "    #存储方向传递需要的值到cache中\n",
    "    cache = (a_next,c_next,a_prev,c_prev,ft,it,cct,ot,xt,parameters)\n",
    "    \n",
    "    return a_next,c_next,yt_pred,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
      " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "#验证代码\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2-LSTM前向传递\n",
    "多个lstm单元循环组成前向传递\n",
    "<img src=\"images/LSTM_rnn.png\" style=\"width:500;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x,a0,parameters):\n",
    "    '''\n",
    "    完成lstm前向传递\n",
    "    输入：\n",
    "        xt：每一个time_strp的输入数据，数据类型：numpy.array,大小:(n_x,m,T_x)\n",
    "        a0: 初始的异常状态，数据类型：numpy.array，大小:(n_a,m)\n",
    "        paremeters:Python的一个字典类型数据包括：\n",
    "            Wf:遗忘门的权重矩阵，输类型numpy.array，大小：(n_a,n_a+n_x)\n",
    "            bf:遗忘门的偏置，输类型numpy.array，大小：(n_a,1)\n",
    "            Wi:乘以输入的权重矩阵，数据类型numpy.array,大小：(n_a,n_x+n_a)\n",
    "            Wi:更新门的偏置，数据类型numpy.array,大小：(n_a,1)\n",
    "            Wc:第一个tanh的权重，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            bc:第一个tanh的偏置，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            Wo:输出门的权重，数据类型numpy.array,大小(n_a,n_a+n_x)\n",
    "            bo:输出门的偏置，数据类型numpy.array,大小(n_a,1)\n",
    "            Wy:隐藏层到输出层的权重，数据类型numpy.array,大小(n_y,n_a)\n",
    "            by:隐藏层到输出层的偏置，数据类型numpy.array,大小(n_y,n_a)\n",
    "    输出：\n",
    "        a:每一个time_step的隐藏状态，(n_a,m,T_x)\n",
    "        y:每一个time_step的预测值，(n_y,m,T_x)\n",
    "        caches:方向传递需要的元组值包含(caches,x)\n",
    "    '''\n",
    "    #初始化caches\n",
    "    caches = []\n",
    "    \n",
    "    #提取维度参数\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_y,n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    #初始化a,c,y\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    c = np.zeros([n_a,m,T_x])\n",
    "    y = np.zeros([n_y,m,T_x])\n",
    "    \n",
    "    #初始化a_next和c_next\n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a,m])\n",
    "    \n",
    "    #循环\n",
    "    for t in range(T_x):\n",
    "        #调用lstm_cell_forward计算a_next,c_next,yt,cache\n",
    "        a_next,c_next,yt,cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)\n",
    "        #保存a_next\n",
    "        a[:,:,t] = a_next\n",
    "        #保存yt\n",
    "        y[:,:,t] = yt\n",
    "        #保存c_next\n",
    "        c[:,:,t] = c_next\n",
    "        #在caches添加cache\n",
    "        caches.append(cache)\n",
    "        \n",
    "    #保存方向传递需要用的caches\n",
    "    caches = (caches,x)\n",
    "    \n",
    "    return a,y,c,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181981\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "#代码测试\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-反方向传递\n",
    "和经典的神经网络有些许不同，RNN在方向传递是也需要沿着time_step方向传递\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next,cache):\n",
    "    '''\n",
    "    完成rnn单元的反向传递\n",
    "    输入：\n",
    "        da_next:下一个隐藏层的损失梯度\n",
    "        cache:python的字典（同前向传递）\n",
    "    输出：\n",
    "        gradients:python字典结构数据\n",
    "            dx:数据数据梯度,(n_x,m)\n",
    "            da_prev:前一个隐藏状态的梯度(n_a,m)\n",
    "            dWax:输入层到隐藏层的犬权重梯度,(n_a,n_x)\n",
    "            dWaa:隐藏层到隐藏层的权重梯度,(n_a,n_a)\n",
    "            dba:偏置的梯度,(n_a,1)\n",
    "    '''\n",
    "    #提取cache参数值\n",
    "    (a_next,a_prev,xt,parameters) = cache\n",
    "    \n",
    "    #从parameters中提取权重\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    #计算tanh关于a_next的梯度\n",
    "    dtanh = (1-a_next*a_next)*da_next\n",
    "    \n",
    "    #计算损失关于Wax的梯度\n",
    "    dxt = np.dot(Wax.T,dtanh)\n",
    "    dWax = np.dot(dtanh,xt.T)\n",
    "    \n",
    "    #计算关于Waa的梯度\n",
    "    da_prev = np.dot(Waa.T,dtanh)\n",
    "    dWaa = np.dot(dtanh,da_prev.T)\n",
    "    \n",
    "    #计算关于b的梯度\n",
    "    dba = np.sum(dtanh,keepdims=True,axis=-1)\n",
    "    \n",
    "    #将梯度存储为字典值\n",
    "    gradients = {\"dxt\":dxt,\"da_prev\":da_prev,\"dWax\":dWax,\"dWaa\":dWaa,\"dba\":dba}\n",
    "    \n",
    "    return gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = -1.3872130506020928\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = -0.15239949377395473\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 0.41077282493545836\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 1.0470184237718851\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [0.20023491]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "#检验\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da,caches):\n",
    "    '''\n",
    "    完成RNN的反向传递\n",
    "    输入：\n",
    "        da:所有隐藏状态的梯度(n_a,m,T_x)\n",
    "        caches:前向传递信息的元组\n",
    "    输出：\n",
    "        gradients:python字典结构数据\n",
    "            dx:输入数据梯度,(n_x,m,T_x)\n",
    "            da0:初始隐藏状态的梯度(n_a,m)\n",
    "            dWax:输入层到隐藏层的犬权重梯度,(n_a,n_x)\n",
    "            dWaa:隐藏层到隐藏层的权重梯度,(n_a,n_a)\n",
    "            dba:偏置的梯度,(n_a,1)\n",
    "    '''\n",
    "    #去caches中的值\n",
    "    (caches,x) = caches\n",
    "    (a1,a0,x1,parameters) = caches[0]\n",
    "    \n",
    "    #提取da和x1的维度\n",
    "    n_a,m,T_x = da.shape\n",
    "    n_x,m = x1.shape\n",
    "    \n",
    "    #初始化梯度\n",
    "    dx = np.zeros([n_x,m,T_x])\n",
    "    dWax = np.zeros([n_a,n_x])\n",
    "    dWaa = np.zeros([n_a,n_a])\n",
    "    dba = np.zeros([n_a,1])\n",
    "    da0 = np.zeros([n_a,m])\n",
    "    da_prevt = np.zeros([n_a,m])\n",
    "    \n",
    "    #循环\n",
    "    for t in reversed(range(T_x)):\n",
    "        #调用rnn_cell_backward计算\n",
    "        gradients = rnn_cell_backward(da[:,:,t]+da_prevt,caches[t])\n",
    "        dxt = gradients[\"dxt\"]\n",
    "        da_prevt = gradients[\"da_prev\"]\n",
    "        dWaxt = gradients[\"dWax\"]\n",
    "        dWaat = gradients[\"dWaa\"]\n",
    "        dbat = gradients[\"dba\"]\n",
    "        \n",
    "        dx[:,:,t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "    \n",
    "    da0 = da_prevt\n",
    "    \n",
    "    #存储梯度在一个字典中\n",
    "    gradients = {\"dx\":dx,\"da0\":da0,\"dWax\":dWax,\"dWaa\":dWaa,\"dba\":dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.31494237512664996\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 11.264104496527777\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 5.6088427884141385\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [-0.74747722]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
